{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Copy of Hans_on_Session_Tools_MT_Summit.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UaVPk0NGLIm"
      },
      "source": [
        "In this session, we will walk through the process of extracting features using a corpus drawn from the children's tale by Lewis Carroll  Alice's Adventures in Wonderland. The corpus is composed of a source text in English with 260 sentences that were all aligned with their human translation into Portuguese, with the raw MT output in Portuguese (Google Translate output) and the post-edited version of the raw MT output.\n",
        "\n",
        "To extract the features from all text types, you will work with the 6 Python programs below. With these programs, you can extract the following features from the texts:\n",
        "\n",
        "**Lexical density, Lexical variety, Mean sentence length, Sentence count, Pronoun ratio, Length ratio, Punctuation ratio**\n",
        "\n",
        "You can analyze all features or, **at least, two features.**\n",
        "\n",
        "\n",
        "Download the spreadsheet to enter your data: https://docs.google.com/spreadsheets/d/1vo_LKkqu8gWYFxD7JDevoauzL9JB_u_v/edit?usp=sharing&ouid=112972086900421300502&rtpof=true&sd=true\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVVNhQ5Ga-do"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0bysFe9L1e4"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJOiT7lyqJPX"
      },
      "source": [
        "# FIx Encodind Program"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2WC-v1rqHF6"
      },
      "source": [
        "import html\n",
        "\n",
        "with open('/content/TGOTT_Cris.per.txt', 'r') as f:\n",
        "    html_text = f.read()\n",
        "\n",
        "html_text = html.unescape(html_text)\n",
        "\n",
        "with open('/content/TGOTT_Cris.per_fixed.txt', 'w') as f:\n",
        "    f.write(html_text)\n",
        "html_text.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG-lYv1rafOF"
      },
      "source": [
        "\n",
        "\n",
        "# Prints *Lexical Density* (number of content words divided by the total number of words) sentence by sentence and average sentence lexical density. **ATTENTION:** To run this program you will need to enter **PoS** files. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DLnN90LBsj4"
      },
      "source": [
        "from google.colab import drive #execute this code to mount your drive, open the link, copy the code and past it to the text box and press enter. \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGCDneg5afOI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e9846f4-7053-459b-c60d-2438511494b7"
      },
      "source": [
        "\n",
        "path = '/content/PoS_Alice_PT_PE.txt'\n",
        "sentencas=open(path, 'r')\n",
        "sentences=sentencas.read()\n",
        "units=sentences.split('SPACE')    \n",
        "    \n",
        "lista_Ldensity=[]\n",
        "print(\"this file contains\", len(units), \"sentences\")\n",
        "\n",
        "for sentence_num, unit in enumerate(units, 1):\n",
        "    lines=unit.split('\\n')\n",
        "    total_count=len(lines)\n",
        "    lexical_count=0\n",
        "    for line in lines:\n",
        "        spl=line.split()\n",
        "        #print(spl)\n",
        "        for j in spl:\n",
        "            if j =='VERB':\n",
        "                lexical_count += 1\n",
        "            elif j == 'ADJ':\n",
        "                lexical_count += 1\n",
        "            elif j == 'ADV':\n",
        "                lexical_count += 1\n",
        "            elif j == 'NOUN':\n",
        "                lexical_count += 1\n",
        "    density = lexical_count/ (total_count-3)\n",
        "    lista_Ldensity.append(density)\n",
        "    print(f'Sentence {sentence_num}, Lexical Density is: {density}')\n",
        "\n",
        "\n",
        "print('The average sentence lexical density is: ', sum(lista_Ldensity)/len(lista_Ldensity)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this file contains 250 sentences\n",
            "Sentence 1, Lexical Density is: 0.8\n",
            "Sentence 2, Lexical Density is: 0.5714285714285714\n",
            "Sentence 3, Lexical Density is: 0.36507936507936506\n",
            "Sentence 4, Lexical Density is: 0.47619047619047616\n",
            "Sentence 5, Lexical Density is: 0.5714285714285714\n",
            "Sentence 6, Lexical Density is: 0.6666666666666666\n",
            "Sentence 7, Lexical Density is: 0.4188034188034188\n",
            "Sentence 8, Lexical Density is: 0.5217391304347826\n",
            "Sentence 9, Lexical Density is: 0.5\n",
            "Sentence 10, Lexical Density is: 0.47058823529411764\n",
            "Sentence 11, Lexical Density is: 0.5185185185185185\n",
            "Sentence 12, Lexical Density is: 0.48148148148148145\n",
            "Sentence 13, Lexical Density is: 0.39285714285714285\n",
            "Sentence 14, Lexical Density is: 0.4444444444444444\n",
            "Sentence 15, Lexical Density is: 0.4666666666666667\n",
            "Sentence 16, Lexical Density is: 0.375\n",
            "Sentence 17, Lexical Density is: 0.5\n",
            "Sentence 18, Lexical Density is: 0.3548387096774194\n",
            "Sentence 19, Lexical Density is: 0.35714285714285715\n",
            "Sentence 20, Lexical Density is: 0.4936708860759494\n",
            "Sentence 21, Lexical Density is: 0.56\n",
            "Sentence 22, Lexical Density is: 0.375\n",
            "Sentence 23, Lexical Density is: 0.5\n",
            "Sentence 24, Lexical Density is: 0.4\n",
            "Sentence 25, Lexical Density is: 0.5172413793103449\n",
            "Sentence 26, Lexical Density is: 0.4\n",
            "Sentence 27, Lexical Density is: 0.3076923076923077\n",
            "Sentence 28, Lexical Density is: 0.5625\n",
            "Sentence 29, Lexical Density is: 0.5\n",
            "Sentence 30, Lexical Density is: 0.4838709677419355\n",
            "Sentence 31, Lexical Density is: 0.42857142857142855\n",
            "Sentence 32, Lexical Density is: 0.4\n",
            "Sentence 33, Lexical Density is: 0.5\n",
            "Sentence 34, Lexical Density is: 0.5\n",
            "Sentence 35, Lexical Density is: 0.4444444444444444\n",
            "Sentence 36, Lexical Density is: 0.34210526315789475\n",
            "Sentence 37, Lexical Density is: 0.4\n",
            "Sentence 38, Lexical Density is: 0.410958904109589\n",
            "Sentence 39, Lexical Density is: 0.5\n",
            "Sentence 40, Lexical Density is: 0.5\n",
            "Sentence 41, Lexical Density is: 0.48717948717948717\n",
            "Sentence 42, Lexical Density is: 0.4489795918367347\n",
            "Sentence 43, Lexical Density is: 0.4375\n",
            "Sentence 44, Lexical Density is: 0.4807692307692308\n",
            "Sentence 45, Lexical Density is: 0.46511627906976744\n",
            "Sentence 46, Lexical Density is: 0.45614035087719296\n",
            "Sentence 47, Lexical Density is: 0.4166666666666667\n",
            "Sentence 48, Lexical Density is: 0.38461538461538464\n",
            "Sentence 49, Lexical Density is: 0.5357142857142857\n",
            "Sentence 50, Lexical Density is: 0.5172413793103449\n",
            "Sentence 51, Lexical Density is: 0.4315068493150685\n",
            "Sentence 52, Lexical Density is: 0.45901639344262296\n",
            "Sentence 53, Lexical Density is: 0.3\n",
            "Sentence 54, Lexical Density is: 0.475\n",
            "Sentence 55, Lexical Density is: 0.36538461538461536\n",
            "Sentence 56, Lexical Density is: 0.42857142857142855\n",
            "Sentence 57, Lexical Density is: 0.4631578947368421\n",
            "Sentence 58, Lexical Density is: 0.3870967741935484\n",
            "Sentence 59, Lexical Density is: 0.4583333333333333\n",
            "Sentence 60, Lexical Density is: 0.5\n",
            "Sentence 61, Lexical Density is: 0.5\n",
            "Sentence 62, Lexical Density is: 0.37755102040816324\n",
            "Sentence 63, Lexical Density is: 0.4666666666666667\n",
            "Sentence 64, Lexical Density is: 0.4805194805194805\n",
            "Sentence 65, Lexical Density is: 0.45454545454545453\n",
            "Sentence 66, Lexical Density is: 0.5\n",
            "Sentence 67, Lexical Density is: 0.4864864864864865\n",
            "Sentence 68, Lexical Density is: 0.4642857142857143\n",
            "Sentence 69, Lexical Density is: 0.35\n",
            "Sentence 70, Lexical Density is: 0.7142857142857143\n",
            "Sentence 71, Lexical Density is: 0.3902439024390244\n",
            "Sentence 72, Lexical Density is: 0.375\n",
            "Sentence 73, Lexical Density is: 0.45454545454545453\n",
            "Sentence 74, Lexical Density is: 0.34615384615384615\n",
            "Sentence 75, Lexical Density is: 0.5\n",
            "Sentence 76, Lexical Density is: 0.5555555555555556\n",
            "Sentence 77, Lexical Density is: 0.45652173913043476\n",
            "Sentence 78, Lexical Density is: 0.6666666666666666\n",
            "Sentence 79, Lexical Density is: 0.42105263157894735\n",
            "Sentence 80, Lexical Density is: 0.2972972972972973\n",
            "Sentence 81, Lexical Density is: 0.3\n",
            "Sentence 82, Lexical Density is: 0.40540540540540543\n",
            "Sentence 83, Lexical Density is: 0.41379310344827586\n",
            "Sentence 84, Lexical Density is: 0.43137254901960786\n",
            "Sentence 85, Lexical Density is: 0.25\n",
            "Sentence 86, Lexical Density is: 0.42857142857142855\n",
            "Sentence 87, Lexical Density is: 0.45161290322580644\n",
            "Sentence 88, Lexical Density is: 0.5\n",
            "Sentence 89, Lexical Density is: 0.4444444444444444\n",
            "Sentence 90, Lexical Density is: 0.3\n",
            "Sentence 91, Lexical Density is: 0.46153846153846156\n",
            "Sentence 92, Lexical Density is: 0.38461538461538464\n",
            "Sentence 93, Lexical Density is: 0.3684210526315789\n",
            "Sentence 94, Lexical Density is: 0.4444444444444444\n",
            "Sentence 95, Lexical Density is: 0.28125\n",
            "Sentence 96, Lexical Density is: 0.4262295081967213\n",
            "Sentence 97, Lexical Density is: 0.24\n",
            "Sentence 98, Lexical Density is: 0.35714285714285715\n",
            "Sentence 99, Lexical Density is: 0.36\n",
            "Sentence 100, Lexical Density is: 0.3\n",
            "Sentence 101, Lexical Density is: 0.5\n",
            "Sentence 102, Lexical Density is: 0.5384615384615384\n",
            "Sentence 103, Lexical Density is: 0.2857142857142857\n",
            "Sentence 104, Lexical Density is: 0.5111111111111111\n",
            "Sentence 105, Lexical Density is: 0.6666666666666666\n",
            "Sentence 106, Lexical Density is: 0.6\n",
            "Sentence 107, Lexical Density is: 0.5\n",
            "Sentence 108, Lexical Density is: 0.4\n",
            "Sentence 109, Lexical Density is: 0.25\n",
            "Sentence 110, Lexical Density is: 0.8\n",
            "Sentence 111, Lexical Density is: 0.6\n",
            "Sentence 112, Lexical Density is: 0.6\n",
            "Sentence 113, Lexical Density is: 0.46774193548387094\n",
            "Sentence 114, Lexical Density is: 0.3684210526315789\n",
            "Sentence 115, Lexical Density is: 0.5294117647058824\n",
            "Sentence 116, Lexical Density is: 0.46153846153846156\n",
            "Sentence 117, Lexical Density is: 0.39285714285714285\n",
            "Sentence 118, Lexical Density is: 0.6666666666666666\n",
            "Sentence 119, Lexical Density is: 0.5172413793103449\n",
            "Sentence 120, Lexical Density is: 0.36363636363636365\n",
            "Sentence 121, Lexical Density is: 0.25\n",
            "Sentence 122, Lexical Density is: 0.45161290322580644\n",
            "Sentence 123, Lexical Density is: 0.4519230769230769\n",
            "Sentence 124, Lexical Density is: 0.4166666666666667\n",
            "Sentence 125, Lexical Density is: 0.46153846153846156\n",
            "Sentence 126, Lexical Density is: 0.38235294117647056\n",
            "Sentence 127, Lexical Density is: 0.4393939393939394\n",
            "Sentence 128, Lexical Density is: 0.47619047619047616\n",
            "Sentence 129, Lexical Density is: 0.5238095238095238\n",
            "Sentence 130, Lexical Density is: 0.35294117647058826\n",
            "Sentence 131, Lexical Density is: 0.4\n",
            "Sentence 132, Lexical Density is: 0.5\n",
            "Sentence 133, Lexical Density is: 0.4838709677419355\n",
            "Sentence 134, Lexical Density is: 0.4117647058823529\n",
            "Sentence 135, Lexical Density is: 0.5\n",
            "Sentence 136, Lexical Density is: 0.3888888888888889\n",
            "Sentence 137, Lexical Density is: 0.5\n",
            "Sentence 138, Lexical Density is: 0.3584905660377358\n",
            "Sentence 139, Lexical Density is: 0.3888888888888889\n",
            "Sentence 140, Lexical Density is: 0.49122807017543857\n",
            "Sentence 141, Lexical Density is: 0.48717948717948717\n",
            "Sentence 142, Lexical Density is: 0.39473684210526316\n",
            "Sentence 143, Lexical Density is: 0.39285714285714285\n",
            "Sentence 144, Lexical Density is: 0.45\n",
            "Sentence 145, Lexical Density is: 0.4444444444444444\n",
            "Sentence 146, Lexical Density is: 0.4636363636363636\n",
            "Sentence 147, Lexical Density is: 0.37037037037037035\n",
            "Sentence 148, Lexical Density is: 0.6153846153846154\n",
            "Sentence 149, Lexical Density is: 0.5\n",
            "Sentence 150, Lexical Density is: 0.45\n",
            "Sentence 151, Lexical Density is: 0.0\n",
            "Sentence 152, Lexical Density is: 0.5555555555555556\n",
            "Sentence 153, Lexical Density is: 0.5\n",
            "Sentence 154, Lexical Density is: 0.4\n",
            "Sentence 155, Lexical Density is: 0.4074074074074074\n",
            "Sentence 156, Lexical Density is: 0.45454545454545453\n",
            "Sentence 157, Lexical Density is: 0.45454545454545453\n",
            "Sentence 158, Lexical Density is: 0.42105263157894735\n",
            "Sentence 159, Lexical Density is: 0.4032258064516129\n",
            "Sentence 160, Lexical Density is: 0.46511627906976744\n",
            "Sentence 161, Lexical Density is: 0.5\n",
            "Sentence 162, Lexical Density is: 0.6666666666666666\n",
            "Sentence 163, Lexical Density is: 0.3888888888888889\n",
            "Sentence 164, Lexical Density is: 0.45454545454545453\n",
            "Sentence 165, Lexical Density is: 0.4626865671641791\n",
            "Sentence 166, Lexical Density is: 0.34615384615384615\n",
            "Sentence 167, Lexical Density is: 0.5\n",
            "Sentence 168, Lexical Density is: 0.3157894736842105\n",
            "Sentence 169, Lexical Density is: 0.4782608695652174\n",
            "Sentence 170, Lexical Density is: 0.4117647058823529\n",
            "Sentence 171, Lexical Density is: 0.5\n",
            "Sentence 172, Lexical Density is: 0.42857142857142855\n",
            "Sentence 173, Lexical Density is: 0.45714285714285713\n",
            "Sentence 174, Lexical Density is: 0.3076923076923077\n",
            "Sentence 175, Lexical Density is: 0.3333333333333333\n",
            "Sentence 176, Lexical Density is: 0.36363636363636365\n",
            "Sentence 177, Lexical Density is: 0.4\n",
            "Sentence 178, Lexical Density is: 0.3191489361702128\n",
            "Sentence 179, Lexical Density is: 0.26666666666666666\n",
            "Sentence 180, Lexical Density is: 0.2962962962962963\n",
            "Sentence 181, Lexical Density is: 0.3870967741935484\n",
            "Sentence 182, Lexical Density is: 0.36363636363636365\n",
            "Sentence 183, Lexical Density is: 0.4666666666666667\n",
            "Sentence 184, Lexical Density is: 0.4444444444444444\n",
            "Sentence 185, Lexical Density is: 0.34615384615384615\n",
            "Sentence 186, Lexical Density is: 0.45454545454545453\n",
            "Sentence 187, Lexical Density is: 0.3870967741935484\n",
            "Sentence 188, Lexical Density is: 0.5\n",
            "Sentence 189, Lexical Density is: 0.4782608695652174\n",
            "Sentence 190, Lexical Density is: 0.47368421052631576\n",
            "Sentence 191, Lexical Density is: 0.4\n",
            "Sentence 192, Lexical Density is: 0.4186046511627907\n",
            "Sentence 193, Lexical Density is: 0.3684210526315789\n",
            "Sentence 194, Lexical Density is: 0.38461538461538464\n",
            "Sentence 195, Lexical Density is: 0.4090909090909091\n",
            "Sentence 196, Lexical Density is: 0.4722222222222222\n",
            "Sentence 197, Lexical Density is: 0.4\n",
            "Sentence 198, Lexical Density is: 0.45454545454545453\n",
            "Sentence 199, Lexical Density is: 0.3888888888888889\n",
            "Sentence 200, Lexical Density is: 0.3125\n",
            "Sentence 201, Lexical Density is: 0.425\n",
            "Sentence 202, Lexical Density is: 0.3333333333333333\n",
            "Sentence 203, Lexical Density is: 0.5277777777777778\n",
            "Sentence 204, Lexical Density is: 0.3333333333333333\n",
            "Sentence 205, Lexical Density is: 0.3333333333333333\n",
            "Sentence 206, Lexical Density is: 0.5454545454545454\n",
            "Sentence 207, Lexical Density is: 0.2777777777777778\n",
            "Sentence 208, Lexical Density is: 0.5\n",
            "Sentence 209, Lexical Density is: 0.4444444444444444\n",
            "Sentence 210, Lexical Density is: 0.47619047619047616\n",
            "Sentence 211, Lexical Density is: 0.5227272727272727\n",
            "Sentence 212, Lexical Density is: 0.4634146341463415\n",
            "Sentence 213, Lexical Density is: 0.37037037037037035\n",
            "Sentence 214, Lexical Density is: 0.325\n",
            "Sentence 215, Lexical Density is: 0.3333333333333333\n",
            "Sentence 216, Lexical Density is: 0.38235294117647056\n",
            "Sentence 217, Lexical Density is: 0.41237113402061853\n",
            "Sentence 218, Lexical Density is: 0.42857142857142855\n",
            "Sentence 219, Lexical Density is: 0.125\n",
            "Sentence 220, Lexical Density is: 0.4090909090909091\n",
            "Sentence 221, Lexical Density is: 0.3333333333333333\n",
            "Sentence 222, Lexical Density is: 0.5263157894736842\n",
            "Sentence 223, Lexical Density is: 0.2222222222222222\n",
            "Sentence 224, Lexical Density is: 0.23529411764705882\n",
            "Sentence 225, Lexical Density is: 0.3333333333333333\n",
            "Sentence 226, Lexical Density is: 0.5\n",
            "Sentence 227, Lexical Density is: 0.3333333333333333\n",
            "Sentence 228, Lexical Density is: 0.5714285714285714\n",
            "Sentence 229, Lexical Density is: 0.4375\n",
            "Sentence 230, Lexical Density is: 0.4358974358974359\n",
            "Sentence 231, Lexical Density is: 0.4166666666666667\n",
            "Sentence 232, Lexical Density is: 0.4117647058823529\n",
            "Sentence 233, Lexical Density is: 0.38461538461538464\n",
            "Sentence 234, Lexical Density is: 0.48\n",
            "Sentence 235, Lexical Density is: 0.3333333333333333\n",
            "Sentence 236, Lexical Density is: 0.35\n",
            "Sentence 237, Lexical Density is: 0.5217391304347826\n",
            "Sentence 238, Lexical Density is: 0.5714285714285714\n",
            "Sentence 239, Lexical Density is: 0.46153846153846156\n",
            "Sentence 240, Lexical Density is: 0.38461538461538464\n",
            "Sentence 241, Lexical Density is: 0.5\n",
            "Sentence 242, Lexical Density is: 0.42105263157894735\n",
            "Sentence 243, Lexical Density is: 0.6\n",
            "Sentence 244, Lexical Density is: 0.38461538461538464\n",
            "Sentence 245, Lexical Density is: 0.4\n",
            "Sentence 246, Lexical Density is: 0.391304347826087\n",
            "Sentence 247, Lexical Density is: 0.3333333333333333\n",
            "Sentence 248, Lexical Density is: 0.5\n",
            "Sentence 249, Lexical Density is: 0.47368421052631576\n",
            "Sentence 250, Lexical Density is: 0.5\n",
            "The average sentence lexical density is:  0.4359486871587956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwrnziwtfbQ9"
      },
      "source": [
        "# Calculate Lexical Richness sentence by sentence and average sentence lexical denisty. **ATTENTION:** To run this program you will need to enter files with **sentences boundaries**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6vAj7J0fe7Q"
      },
      "source": [
        "import re\n",
        "#import nltk\n",
        "\n",
        "\n",
        "sentencas=open('/TGITT-PT_original_sent-boundaries.txt', 'r')\n",
        "sentences=sentencas.read().lower()\n",
        "units=re.findall(r'<<>(.*)<>>',sentences)\n",
        "\n",
        "sents=['a natalia natalia sheila sheila eu vc nos amor isso aquilo', 'ana ana debora fabiano marisa nos']\n",
        "\n",
        "for unit in units:\n",
        "  sents.append(unit)\n",
        "        \n",
        "\n",
        "   \n",
        "lista_ttr=[]\n",
        "\n",
        "sentence_number = 1\n",
        "\n",
        "for sentence in sents:\n",
        "  w=sentence.split()\n",
        "  print(w)\n",
        "  print(len(set(w)))\n",
        "  print(len(w))\n",
        "  ttr=len(set(w))/len(w)\n",
        "  lista_ttr.append(ttr)\n",
        "  print(ttr) #prints ttr of each sentence\n",
        "  \n",
        "\n",
        "print(\"the average sentence lexical richness is: \", sum(lista_ttr)/len(lista_ttr))\n",
        "    \n",
        "            \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDz19j_fasZ_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIHmu5C70JuM"
      },
      "source": [
        "# Prints mean sentence length in characters. **ATTENTION:** to run this program you will need to enter files with **sentences boundaries**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GroSfhQ7wbOx"
      },
      "source": [
        "import re\n",
        "file_to_open=open('#past your file path here', 'r')\n",
        "\n",
        "sentencas=file_to_open.read()\n",
        "\n",
        "units=re.findall(r'<<>(.*)<>>',sentencas)\n",
        "\n",
        "lista_sent=[]\n",
        "sentence_number = 0\n",
        "for u in units:\n",
        "    if not u.isspace():\n",
        "        sentence_number+=1\n",
        "        length_u=len(u)\n",
        "        lista_sent.append(length_u)\n",
        "        #print('this is len', length_u)\n",
        "\n",
        "\n",
        "print('the mean sentence length in characters is', sum(lista_sent)/len(lista_sent))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62RX-rzXB2me"
      },
      "source": [
        "## Calculate length ratio based on mean sentence length (in characters) data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVH7x5fGBtzx"
      },
      "source": [
        "#for length ratio you need to subtract the mean sentence length of the HT from the source and divide by the mean sentence length of the source\n",
        "\n",
        "#SOURCE vs HT\n",
        "mean_length_source= float(#enter number here) \n",
        "mean_length_HT= float(#enter number here) \n",
        "sentence_length_ratio= (mean_length_source-mean_length_HT/mean_length_source)\n",
        "print('length ratio of the source text vs the HT is', sentence_length_ratio)\n",
        "\n",
        "#HT Vs PE\n",
        "mean_length_PE= float(#enter number here)\n",
        "mean_length_HT= float(#enter number here)\n",
        "sentence_length_ratio= (mean_length_HT - mean_length_PE/mean_length_HT)\n",
        "print('length ratio the HT vs the PE translation is', sentence_length_ratio)\n",
        "\n",
        "#MT Vs PE\n",
        "mean_length_MT=float(#enter number here)\n",
        "mean_length_PE=float(#enter number here)\n",
        "sentence_length_ratio= (mean_length_MT - mean_length_PE/mean_length_MT)\n",
        "print('length ratio the MT raw output vs the HT is', sentence_length_ratio)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NilwLnTL0rQY"
      },
      "source": [
        "# Prints punctuation count. ATTENTION: to run this program you will need to enter files with **sentences boundaries**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJy9GNJn1Hdn"
      },
      "source": [
        "#nltk.download(\"book\") install NLTK resources\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize,wordpunct_tokenize\n",
        "from operator import itemgetter\n",
        "\n",
        "\n",
        "file_to_open=open('#past your file path here', 'r')\n",
        "sentencas=file_to_open.read()\n",
        "sent=wordpunct_tokenize(sentencas)\n",
        "\n",
        "\n",
        "punct_dic=[',',':',';','-','_','!','?','??','.','=','+','\"', \"'\",'\"','[',']','{','}','/','|', '#','%','&','$','*','<','>']\n",
        "punc_list=[]\n",
        "\n",
        "\n",
        "\n",
        "for p in sent:\n",
        "  if p in punct_dic:\n",
        "    punc_list.append((p, sent.count(p)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "sorted_list = sorted(frozenset(punc_list), key=itemgetter(1), reverse=True) #descending order\n",
        "\n",
        "print(sorted_list)\n",
        "\n",
        "for x,y in sorted_list:\n",
        "  print(x,y)\n",
        "\n",
        "  #to calculate the ratio, divide the the frequency of each punctuation mark of one text (e.g. HT) by the frequency of each punctuation mark extracted from another text (e.g. PE)\n",
        "  \n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc_Z1ZVNUeHv"
      },
      "source": [
        "#Prints English pronouns frequency. ATTENTION: to run this program you will need to enter files with **sentence boundaries**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SINtA-mmUpwt"
      },
      "source": [
        "\n",
        "import re\n",
        "English_file_to_open=open('#past your file path here', 'r')\n",
        "\n",
        "English_sentencas=English_file_to_open.read().lower()\n",
        "\n",
        "English_sentencas=English_sentencas.split()\n",
        "\n",
        "\n",
        "\n",
        "En_pronouns_list=['he', 'her', 'hers', 'herself', 'him', 'himself', 'i', 'it', 'itself', 'me', 'mine', 'myself', 'one', 'oneself', 'ours', 'ourselves', 'she', 'theirs', 'them', 'themselves', 'they', 'us', 'we', 'you,' 'yourself']\n",
        "En_frequency_pronouns=[]\n",
        "\n",
        "\n",
        "\n",
        "for i in English_sentencas:\n",
        "  if i in En_pronouns_list:\n",
        "    En_frequency_pronouns.append(i)\n",
        "  else:\n",
        "    continue\n",
        "\n",
        "\n",
        "print(En_frequency_pronouns)\n",
        "\n",
        "print(\"this file contains\", len(En_frequency_pronouns), 'English pronouns')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW3QX7mCnkgz"
      },
      "source": [
        "#Prints Portuguese pronouns frequency. ATTENTION: to run this program you will need to enter files with **sentence boundaries**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzn1NR68hQKh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f74367d-2091-4d04-f2c5-d1bba79c6796"
      },
      "source": [
        "import re\n",
        "\n",
        "Portuguese_file_to_open=open('#past your file path here', 'r')\n",
        "\n",
        "Portuguese_sentencas=Portuguese_file_to_open.read().lower()\n",
        "\n",
        "Portuguese_sentencas=Portuguese_sentencas.split()\n",
        "\n",
        "\n",
        "PT_pronouns_list=['ele','ela', 'eles', 'elas', 'dele', 'dela','sua', 'seu','si', 'ti', 'tu', 'tua', 'eu', 'mim', 'meu', 'nosso', 'nossa', 'deles', 'delas', 'nós', 'vocês', 'vós', 'você']\n",
        "PT_frequency_pronouns=[]\n",
        "\n",
        "for j in Portuguese_sentencas:\n",
        "  if j in PT_pronouns_list:\n",
        "    PT_frequency_pronouns.append(j)\n",
        "  else:\n",
        "    continue\n",
        "\n",
        "print(PT_frequency_pronouns)\n",
        "print(\"this file contains\", len(PT_frequency_pronouns), 'Portuguese pronouns')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ela', 'seu', 'ela', 'ela', 'eles', 'mim', 'eu', 'você']\n",
            "this file contains 8 Portuguese pronouns\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3DmjISPi5sW"
      },
      "source": [
        "# Prints pronoun ratio. You will need to use data from English and Portuguese pronoun frequency. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu-jaRyehigJ"
      },
      "source": [
        "#How to calculate the ratio. Example source vs. HT\n",
        "\n",
        "print('(normalised) source to HT ratio is: ', (len(En_frequency_pronouns)-len(PT_frequency_pronouns))/len(En_frequency_pronouns))\n",
        "\n",
        "#the greater the ratio, the greater the difference between texts\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjdPfOFWrK0k"
      },
      "source": [
        "# Prints sentence count and mean sentence length. ATTENTION: to run this program you will need to enter files with **sentence boundaries**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VFWKcZaMEQx"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLq2AHLSrLjg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e168cbc6-8104-4c8d-e923-896d0f9b284f"
      },
      "source": [
        "nltk.download(\"book\") #install NLTK resources\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize,wordpunct_tokenize, word_tokenize\n",
        "\n",
        "file_to_open=open('#past your file path here', 'r', encoding=\"utf-8\")\n",
        "\n",
        "texto=file_to_open.read()\n",
        "\n",
        "#remove sentence boundaries\n",
        "remove_characters = [\"<<>\", \"<>>\"]\n",
        "for character in remove_characters:\n",
        "    texto = texto.replace(character, \"\")\n",
        "\n",
        "\n",
        "sentences = sent_tokenize(texto)\n",
        "\n",
        "sent_number=1\n",
        "\n",
        "wordcounts = []\n",
        "\n",
        "for sentence in sentences:\n",
        "  w = word_tokenize(sentence)\n",
        "  wordcounts.append(len(w))\n",
        "\n",
        "\n",
        "average_wordcount = sum(wordcounts)/len(wordcounts)\n",
        "\n",
        "print('This file contains', len(sentences), 'sentences')\n",
        "print('The longest sentence of this file contains',max(wordcounts), 'tokens')\n",
        "print('The shortest sentence of this file contains',min(wordcounts),'tokens')\n",
        "print('The mean sentence length of this file is: ',average_wordcount)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6e23bcdce4b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwordpunct_tokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfile_to_open\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'#past your file path here'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtexto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_to_open\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '#past your file path here'"
          ]
        }
      ]
    }
  ]
}